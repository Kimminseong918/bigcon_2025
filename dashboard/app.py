# -*- coding: utf-8 -*-
from __future__ import annotations
from pathlib import Path
import io, os, json, datetime, textwrap, re
import numpy as np
import pandas as pd
import streamlit as st
import plotly.express as px
import plotly.graph_objects as go
import requests

# (ì„ íƒ) Gemini
try:
    import google.generativeai as genai
    _HAS_GEMINI = True
except Exception:
    _HAS_GEMINI = False

try:
    from streamlit.runtime.secrets import StreamlitSecretNotFoundError
except Exception:
    class StreamlitSecretNotFoundError(Exception): ...

# -------------------- ê²½ë¡œ --------------------
THIS = Path(__file__).resolve()
APP_DIR = THIS.parent
ROOT = APP_DIR.parent
OUT = ROOT / "outputs"
OUT.mkdir(parents=True, exist_ok=True)  # outputs í´ë” ë³´ì¥

# -------------------- Google Drive URL (secrets ìš°ì„ , fallback ì¡´ì¬) --------------------
try:
    PREDICTIONS_URL       = st.secrets["PREDICTIONS_URL"]
    PREDICTIONS_NAMED_URL = st.secrets["PREDICTIONS_NAMED_URL"]
    MERGED_URL            = st.secrets["MERGED_URL"]
    ALERTS_URL            = st.secrets["ALERTS_URL"]          # signals_alerts_delta.csv
    SIGREC_URL            = st.secrets.get("SIGREC_URL", "")  # (ì„ íƒ) signals_recent_delta.csv
except Exception:
    PREDICTIONS_URL       = "https://drive.google.com/uc?id=1qInDALlRx25MlShIL4yT4GTiqO-qmSWd&export=download"
    PREDICTIONS_NAMED_URL = "https://drive.google.com/uc?id=1oDGLLAtPhvweruKWq2x9DTHC_LSyLG34&export=download"
    MERGED_URL            = "https://drive.google.com/uc?id=1-iPvmfHz3mjhRe95XEoB17Ja0S_zulJm&export=download"
    ALERTS_URL            = "https://drive.google.com/uc?id=1_WdKGUzAK1xaXlxTbpkCfDpyonYQGWBx&export=download"
    SIGREC_URL            = ""

# -------------------- ê³µí†µ: Google Driveì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ --------------------
def _extract_gdrive_id(url_or_id: str) -> str | None:
    """id=â€¦ ë˜ëŠ” /d/â€¦/ í˜•íƒœ ëª¨ë‘ì—ì„œ íŒŒì¼ID ì¶”ì¶œ"""
    if not url_or_id:
        return None
    if "/" not in url_or_id:  # ì´ë¯¸ idë§Œ ì˜¨ ê²½ìš°
        return url_or_id
    m = re.search(r"(?:id=|/d/)([A-Za-z0-9_-]{20,})", url_or_id)
    return m.group(1) if m else None

def _download_from_gdrive(url_or_id: str, out_path: Path) -> bool:
    try:
        file_id = _extract_gdrive_id(url_or_id)
        if not file_id:
            return False
        url = f"https://drive.google.com/uc?export=download&id={file_id}"
        out_path.parent.mkdir(parents=True, exist_ok=True)
        with requests.get(url, stream=True, timeout=120) as r:
            r.raise_for_status()
            with open(out_path, "wb") as f:
                for chunk in r.iter_content(1 << 20):
                    if chunk:
                        f.write(chunk)
        print(f"[OK] downloaded -> {out_path.name}")
        return True
    except Exception as e:
        print(f"[WARN] download failed: {out_path.name} ({e})")
        return False

def ensure_outputs_files(out_dir: Path) -> None:
    """í•„ìš” íŒŒì¼ ì—†ìœ¼ë©´ Driveì—ì„œ ìë™ ë‹¤ìš´ë¡œë“œ (URLì´ ë¹„ì—ˆìœ¼ë©´ ê±´ë„ˆëœ€)"""
    targets = {
        "predictions_latest_both_delta.parquet":       PREDICTIONS_URL,
        "predictions_latest_both_delta_named.parquet": PREDICTIONS_NAMED_URL,
        "merged_indices_monthly.parquet":              MERGED_URL,
        "signals_alerts_delta.csv":                    ALERTS_URL,
        "signals_recent_delta.csv":                    SIGREC_URL,
    }
    for fname, url in targets.items():
        if not url:
            continue
        p = out_dir / fname
        if not p.exists() or p.stat().st_size == 0:
            _download_from_gdrive(url, p)

# ì‹¤ì œ ë‹¤ìš´ë¡œë“œ ìˆ˜í–‰
ensure_outputs_files(OUT)

# -------------------- ì •ì±… íŒŒì¼ íƒìƒ‰ ë³´ì™„ --------------------
def _resolve_policy_file() -> Path | None:
    # ê¸°ë³¸ ì´ë¦„
    candidate = OUT / "ì •ì±…ì§€ì›ê´€ë ¨ë§¤í•‘_251022.xlsx"
    if candidate.exists():
        return candidate
    # outputs í´ë”ì—ì„œ "ì •ì±…"ê³¼ "ë§¤í•‘"ì´ ë“¤ì–´ê°„ ì•„ë¬´ xlsx ì°¾ê¸° (ì´ë¦„ì´ ì•½ê°„ ë‹¬ë¼ë„ ë¡œë”©)
    xls = sorted([p for p in OUT.glob("*.xlsx") if ("ì •ì±…" in p.name and "ë§¤í•‘" in p.name)])
    if xls:
        return xls[0]
    return None

POLICY_XLSX = _resolve_policy_file()

# -------------------- íŒŒì¼ ê²½ë¡œ (ë‹¤ìš´ë¡œë“œ ì´í›„) --------------------
named_candidate = OUT / "predictions_latest_both_delta_named.parquet"
FILE_PRED   = named_candidate if named_candidate.exists() else (OUT / "predictions_latest_both_delta.parquet")
FILE_MERGED = OUT / "merged_indices_monthly.parquet"
FILE_MAPCSV = OUT / "big_data_set1_f.csv"          # (ì„ íƒ) ê°€ë§¹ì ëª… ë§¤í•‘ CSV
FILE_ALERTS = OUT / "signals_alerts_delta.csv"     # (ì„ íƒ) ê²½ê³ ì‚¬ìœ  í…ìŠ¤íŠ¸
FILE_SIGREC = OUT / "signals_recent_delta.csv"     # (ì„ íƒ) ì§€í‘œ delta/gap ê¸°ë°˜ ì„¤ëª…

LOG_DIR = OUT / "ai_logs"; LOG_DIR.mkdir(parents=True, exist_ok=True)
LOG_PATH = LOG_DIR / "ai_explanation_log.jsonl"

# -------------------- UI/ìŠ¤íƒ€ì¼ --------------------
st.set_page_config(page_title="AI ê¸°ë°˜ íì—… ì¡°ê¸°ê²½ë³´ í”Œë«í¼", layout="wide")
st.markdown("""
<style>
.block-container { padding-top: 1.2rem !important; padding-bottom: 1.8rem !important; }
h1.app-title{ font-size: 32px; font-weight: 900; line-height: 1.36; margin: .1rem 0 .8rem 0; }
.badge{display:inline-block;padding:.2rem .5rem;border-radius:8px;font-size:12px;font-weight:800}
.badge-red{background:rgba(239,68,68,.18);color:#ef4444}
.badge-amber{background:rgba(245,158,11,.18);color:#f59e0b}
.badge-emerald{background:rgba(16,185,129,.18);color:#10b981}
.small{font-size:12px;opacity:.75}
.caption-note{font-size:13px; opacity:.92;}
.caption-list{font-size:13px; opacity:.92; padding-left:1.1rem}

/* ê³µë™êµ¬ë§¤ & ë¦¬ì›Œë“œ ì¹´ë“œ ìŠ¤íƒ€ì¼ */
.card-stack { display: grid; row-gap: 12px; }
.card { border-radius: 14px; padding: 16px 18px; border: 1px solid rgba(0,0,0,.06);
        background: #fff; box-shadow: 0 2px 6px rgba(0,0,0,.04); }
.card-title { font-weight: 700; margin-bottom: 6px; }
.card-sub { font-size: 13.5px; opacity: .86; margin-bottom: 12px; }
.card-badge { font-size: 12.5px; opacity: .8; }
.btn { display:inline-block; padding:10px 14px; border-radius:10px; font-weight:800; text-align:center; }
.btn-primary { background:#ea580c1a; border:1px solid #ea580c55; color:#b45309; }
.btn-primary:hover{ background:#ea580c2a; }
.btn-magenta { background:#db27771a; border:1px solid #db277755; color:#9d174d; }
.btn-magenta:hover{ background:#db27772a; }
.section-chip{font-weight:800;opacity:.9}
</style>
""", unsafe_allow_html=True)
st.markdown("<h1 class='app-title'>ğŸ˜€ AI ê¸°ë°˜ íì—… ì¡°ê¸°ê²½ë³´ í”Œë«í¼</h1>", unsafe_allow_html=True)
st.caption("AI ê¸°ë°˜ íì—…ìœ„í—˜ ì˜ˆì¸¡ ë° ë§ì¶¤í˜• ì§€ì› ì‹œìŠ¤í…œ")

# -------------------- ë¡œë”© ìœ í‹¸ --------------------
def _try_read_csv(path: Path) -> pd.DataFrame:
    for enc in ("cp949", "euc-kr", "utf-8"):
        try:
            return pd.read_csv(path, encoding=enc)
        except Exception:
            pass
    try:
        txt = path.read_bytes().decode("utf-8", errors="replace")
        return pd.read_csv(io.StringIO(txt))
    except Exception:
        return pd.DataFrame()

@st.cache_data(show_spinner=False)
def load_pred() -> pd.DataFrame:
    p = FILE_PRED
    if p.suffix.lower()==".parquet":
        return pd.read_parquet(p)
    return _try_read_csv(p)

@st.cache_data(show_spinner=False)
def load_merged() -> pd.DataFrame:
    if FILE_MERGED.exists():
        return pd.read_parquet(FILE_MERGED) if FILE_MERGED.suffix.lower()==".parquet" else _try_read_csv(FILE_MERGED)
    return pd.DataFrame()

@st.cache_data(show_spinner=False)
def load_mapping() -> pd.DataFrame:
    if not FILE_MAPCSV.exists(): return pd.DataFrame()
    df = _try_read_csv(FILE_MAPCSV)
    id_col  = next((c for c in df.columns if str(c).upper() in {"ENCODED_MCT","ENCODED","STORE_ID"}), None)
    nm_col  = next((c for c in df.columns if str(c).upper() in {"MCT_NM","STORE_NAME","ê°€ë§¹ì ëª…"}), None)
    bse_col = next((c for c in df.columns if "MCT_BSE" in str(c).upper()), None)
    if not id_col or not nm_col: return pd.DataFrame()
    m = df[[id_col, nm_col] + ([bse_col] if bse_col else [])].copy()
    m.columns = ["ENCODED_MCT","MCT_NM"] + (["MCT_BSE_AR"] if bse_col else [])
    m["ENCODED_MCT"] = m["ENCODED_MCT"].astype(str).str.strip()
    m["MCT_NM"] = m["MCT_NM"].astype(str).str.strip()
    if "MCT_BSE_AR" in m.columns: m["MCT_BSE_AR"] = m["MCT_BSE_AR"].astype(str).str.strip()
    return m.drop_duplicates()

@st.cache_data(show_spinner=False)
def load_alerts() -> pd.DataFrame:
    return _try_read_csv(FILE_ALERTS) if FILE_ALERTS.exists() else pd.DataFrame()

@st.cache_data(show_spinner=False)
def load_sigrec() -> pd.DataFrame:
    return _try_read_csv(FILE_SIGREC) if FILE_SIGREC.exists() else pd.DataFrame()

@st.cache_data(show_spinner=False)
def load_policy_map() -> pd.DataFrame:
    if POLICY_XLSX is None or not Path(POLICY_XLSX).exists(): 
        return pd.DataFrame()
    try: df = pd.read_excel(POLICY_XLSX)
    except Exception: 
        df = pd.read_excel(POLICY_XLSX, engine="openpyxl")
    colmap = {}
    for c in df.columns:
        cu = str(c).strip().lower()
        if cu in {"policy_id","id","ì •ì±…id","pid"}: colmap[c] = "policy_id"
        elif cu in {"title","ì •ì±…ëª…","name"}: colmap[c] = "title"
        elif cu in {"owner","ê¸°ê´€","ì£¼ê´€ê¸°ê´€"}: colmap[c] = "owner"
        elif cu in {"url","link","ì‹ ì²­ë§í¬"}: colmap[c] = "url"
        elif cu in {"deadline","ë§ˆê°","ë§ˆê°ì¼"}: colmap[c] = "deadline"
        elif cu in {"support_type","ìœ í˜•","type","ì§€ì›ìœ í˜•"}: colmap[c] = "support_type"
        elif cu in {"region","ì§€ì—­","í–‰ì •ë™","ì§€ìì²´"}: colmap[c] = "region"
        elif cu in {"industry","ì—…ì¢…"}: colmap[c] = "industry"
        elif cu in {"min_risk","ìµœì†Œë“±ê¸‰","ìœ„í—˜ë“±ê¸‰"}: colmap[c] = "min_risk"
        elif cu in {"driver_tags","risk_tags","íƒœê·¸","tags"}: colmap[c] = "risk_tags"
        elif cu in {"risk_group","ìœ„í—˜êµ°","group"}: colmap[c] = "risk_group"
        elif cu in {"summary","ì„¤ëª…","ë¹„ê³ "}: colmap[c] = "summary"
    df = df.rename(columns=colmap)
    for k in ["policy_id","title","owner","url","deadline","support_type","region","industry","min_risk","risk_tags","risk_group","summary"]:
        if k not in df.columns: df[k] = ""
    df["policy_id"] = df["policy_id"].astype(str).str.strip()
    df["title"] = df["title"].astype(str).str.strip()
    df["owner"] = df["owner"].astype(str).str.strip()
    df["url"] = df["url"].astype(str).str.strip()
    df["support_type"] = df["support_type"].astype(str).str.strip().str.lower()
    df["region"] = df["region"].astype(str).str.strip()
    df["industry"] = df["industry"].astype(str).str.strip()
    df["min_risk"] = df["min_risk"].astype(str).str.strip()
    df["risk_group"] = df["risk_group"].astype(str).str.strip().str.lower()
    df["risk_tags"] = (df.get("risk_tags","").astype(str) if "risk_tags" in df else "")
    try: df["deadline"] = pd.to_datetime(df["deadline"], errors="coerce")
    except Exception: df["deadline"] = pd.NaT
    def _infer_support_type(row):
        t = (row.get("support_type") or "").strip().lower()
        if t: return t
        g = (row.get("risk_group") or "").lower()
        if any(k in g for k in ["ëŒ€ì¶œ","ê¸ˆìœµ","ë¶€ë‹´","ê¸ˆë¦¬"]): return "loan"
        if any(k in g for k in ["ë³´í—˜"]): return "insurance"
        if any(k in g for k in ["ê³ ê°","ë§ˆì¼€íŒ…","í™ë³´","íŒë¡œ"]): return "marketing"
        if any(k in g for k in ["ê³µë™êµ¬ë§¤","ì›ê°€","ì„ëŒ€","ë¹„ìš©"]): return "sourcing"
        return "policy"
    df["support_type"] = df.apply(_infer_support_type, axis=1)
    return df

def _ensure_datetime(df: pd.DataFrame, col="month"):
    if col in df.columns and not np.issubdtype(df[col].dtype, np.datetime64):
        df[col] = pd.to_datetime(df[col], errors="coerce")

# -------------------- ë°ì´í„° ì ì¬/ì •ë¦¬ --------------------
pred   = load_pred(); merged = load_merged(); mapping = load_mapping()
alerts = load_alerts(); sigrec  = load_sigrec(); policy_map = load_policy_map()
for _df in (pred, merged, alerts, sigrec):
    if not _df.empty: _ensure_datetime(_df, "month")

# í‘œì¤€ í‚¤/ì´ë¦„ ë§¤í•‘
if "store_id" in pred.columns: pred["store_id"] = pred["store_id"].astype(str)
if "ENCODED_MCT" not in pred.columns and "store_id" in pred.columns:
    pred["ENCODED_MCT"] = pred["store_id"]
if not mapping.empty and {"ENCODED_MCT","MCT_NM"}.issubset(mapping.columns):
    pred = pred.merge(mapping, on="ENCODED_MCT", how="left")
name = pred.get("MCT_NM", pd.Series(index=pred.index, dtype=object))
name = name.astype(str).str.strip().replace({"nan":"","None":"","NULL":"","<NA>":""})
pred["MCT_NM"] = np.where(name.eq(""), pred["store_id"], name)

# -------------------- ê³µí†µ ìœ í‹¸/ì„¤ëª… --------------------
def latest_month(df: pd.DataFrame) -> pd.Timestamp | None:
    return pd.to_datetime(df["month"]).max() if "month" in df else None

def latest_per_store(df: pd.DataFrame) -> pd.DataFrame:
    if "month" not in df: return df.copy()
    idx = df.groupby("store_id", observed=False)["month"].idxmax()
    return df.loc[idx].copy()

def pct(n, d):
    try: return f"{100*n/d:.1f}%"
    except Exception: return "N/A"

def num(x):
    try: return f"{int(x):,}"
    except Exception: return "0"

def _kdict():
    return {"customer":"ê³ ê°ì¸µ ë³€í™”","market":"ìƒê¶Œ í˜¼ì¡Â·íì—…ë¥ ","industry":"ì—…ì¢… ìœ„í—˜","sales":"ë§¤ì¶œ íë¦„","macro":"ê±°ì‹œÂ·ì„ëŒ€ë£Œ"}

def _intro_text(kind: str) -> str:
    msg = {
        "avg3":"<div class='callout'><b>ì „ì²´ ì í¬ì˜ ì›”ë³„ í‰ê·  â€˜3ê°œì›” í›„ íì—… ìœ„í—˜ í™•ë¥ â€™</b>ì…ë‹ˆë‹¤.</div>",
        "avg6":"<div class='callout'><b>ì „ì²´ ì í¬ì˜ ì›”ë³„ í‰ê·  â€˜6ê°œì›” í›„ íì—… ìœ„í—˜ í™•ë¥ â€™</b>ì…ë‹ˆë‹¤.</div>",
        "shop3":"<div class='callout'>ì„ íƒí•œ ì í¬ì˜ <b>ì›”ë³„ â€˜3ê°œì›” í›„ íì—… ìœ„í—˜ í™•ë¥ â€™</b> ì¶”ì„¸ì…ë‹ˆë‹¤.</div>",
        "shop6":"<div class='callout'>ì„ íƒí•œ ì í¬ì˜ <b>ì›”ë³„ â€˜6ê°œì›” í›„ íì—… ìœ„í—˜ í™•ë¥ â€™</b> ì¶”ì„¸ì…ë‹ˆë‹¤.</div>",
    }
    return msg.get(kind,"")

def _describe_ts(months: pd.Series, values: pd.Series, scope_label: str) -> str:
    try:
        m = pd.to_datetime(months); v = pd.to_numeric(values, errors="coerce")
        ok = m.notna() & v.notna(); m, v = m[ok], v[ok]
        if len(v) < 2: return f"<div class='caption-note'>Â· {scope_label}: ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.</div>"
        last, last_m = float(v.iloc[-1]), m.iloc[-1]
        w = 3 if len(v) >= 4 else (len(v)-1)
        recent_delta = last - float(v.iloc[-(w+1)])
        idx_max, idx_min = int(v.argmax()), int(v.argmin())
        m_max, v_max = m.iloc[idx_max], float(v.iloc[idx_max]); m_min, v_min = m.iloc[idx_min], float(v.iloc[idx_min])
        def p(x): return f"{x*100:.1f}%"
        def sign_txt(x):
            return f"ìƒìŠ¹(+{p(x)})" if x>0.0001 else (f"í•˜ë½({p(x)})" if x<-0.0001 else "í° ë³€í™” ì—†ìŒ(Â±0.0%p)")
        return "".join([
            f"<div class='caption-note'><b>ê·¸ë˜í”„ ìš”ì•½</b> â€” {scope_label}</div>",
            "<ul class='caption-list'>",
            f"<li><b>í˜„ì¬</b>: {last_m.strftime('%Y-%m')} ê¸°ì¤€ {p(last)} ì…ë‹ˆë‹¤.</li>",
            f"<li><b>ìµœê·¼</b>: ìµœê·¼ {w}ê°œì›” {sign_txt(recent_delta)}.</li>",
            f"<li><b>ë²”ìœ„</b>: ìµœê³  {p(v_max)}({m_max.strftime('%Y-%m')}), ìµœì € {p(v_min)}({m_min.strftime('%Y-%m')}).</li>",
            "</ul>"
        ])
    except Exception:
        return f"<div class='caption-note'>Â· {scope_label}: í•´ì„ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.</div>"

# === build_ai_prompt ===
def build_ai_prompt(store_name: str, store_id: str, district: str, category: str,
                    top_groups: list[str], reasons: list[str], score_now: float,
                    extra_metrics: dict) -> str:
    lines = []
    lines.append(f"[ì í¬] ì´ë¦„: {store_name} / ID: {store_id}")
    if district or category:
        lines.append(f"[ë©”íƒ€] í–‰ì •ë™: {district or '-'} / ì—…ì¢…: {category or '-'}")
    if np.isfinite(score_now): lines.append(f"[í˜„ì¬ ìœ„í—˜í™•ë¥ (3M)] {score_now*100:.1f}%")
    if top_groups: lines.append("[ì˜í–¥ ê·¸ë£¹ Top3] " + " Â· ".join(top_groups))
    if reasons: lines.append("[ì›ì¸ ì‹ í˜¸] " + " / ".join([r.lstrip('- ').strip() for r in reasons]))
    if extra_metrics: lines.append("[ìš”ì•½ ìˆ˜ì¹˜] " + " / ".join([f"{k}: {v}" for k,v in extra_metrics.items()]))
    system = textwrap.dedent("""
    ì‘ì—…: ì£¼ì–´ì§„ ì í¬ ìœ„í—˜ìš”ì•½ì„ 2~3ì¤„ë¡œ ì„¤ëª…í•˜ì„¸ìš”.
    - ì™œ ì•Œë¦¼ì´ ë–´ëŠ”ì§€(í•µì‹¬ ì›ì¸)
    - ì§€ê¸ˆ ì í¬ì£¼ê°€ í•  ì¼(CTA, í–‰ë™ 1~2ê°œ)
    ë¬¸ì²´: ê°„ê²°, ì‹¤í–‰ì§€í–¥, ìˆ«ìëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš©.
    """).strip()
    return system + "\n\n" + "\n".join(lines)

def _save_ai_log(store_id: str, prompt: str, response: str, metrics: dict):
    LOG_DIR.mkdir(parents=True, exist_ok=True)
    with open(LOG_PATH, "a", encoding="utf-8") as f:
        f.write(json.dumps({
            "timestamp": datetime.datetime.now().isoformat(),
            "store_id": store_id, "prompt": prompt, "response": response, "metrics": metrics
        }, ensure_ascii=False) + "\n")

# ------------- Gemini í‚¤/ëª¨ë¸ -------------
def _get_gemini_key_from_user() -> str | None:
    key = None
    try:
        if hasattr(st, "secrets"): key = st.secrets.get("GEMINI_API_KEY", None)
    except StreamlitSecretNotFoundError: key = None
    if not key: key = os.getenv("GEMINI_API_KEY")
    if not key: key = st.session_state.get("GEMINI_API_KEY")
    if not key:
        with st.popover("ğŸ” Gemini API í‚¤ ì…ë ¥", use_container_width=True):
            st.caption("Â· ê¶Œì¥: .streamlit/secrets.toml ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©", unsafe_allow_html=True)
            _k = st.text_input("GEMINI_API_KEY", type="password", placeholder="AIza...", label_visibility="collapsed")
            if _k: st.session_state["GEMINI_API_KEY"] = key = _k.strip(); st.success("í‚¤ê°€ ì„¸ì…˜ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return key

def _list_models_safe():
    if not _HAS_GEMINI: return []
    try:
        models=[]
        for m in genai.list_models():
            name = getattr(m,"name",None) or getattr(m,"model",None)
            methods = set(getattr(m,"supported_generation_methods",[]) or [])
            if not methods:
                caps = getattr(m,"generation_capabilities",None)
                if isinstance(caps,dict): methods = set(caps.get("methods",[]))
            models.append((name,methods))
        return models
    except Exception:
        return []

def _pick_gemini_model() -> str:
    preferred = [
        "models/gemini-2.5-flash","models/gemini-2.5-pro",
        "models/gemini-1.5-flash-latest","models/gemini-1.5-flash",
        "models/gemini-1.5-pro","models/gemini-1.0-pro",
    ]
    avail = _list_models_safe()
    def ok(ms):
        s={str(x).lower() for x in (ms or set())}
        return any(k in s for k in ("generatecontent","generate_content"))
    text_models=[n for (n,ms) in avail if n and ok(ms)]
    for m in preferred:
        if m in text_models: return m
    return text_models[0] if text_models else "models/gemini-2.5-flash"

def _gemini_generate(prompt: str) -> str:
    key = _get_gemini_key_from_user()
    if not _HAS_GEMINI: return "[ì„¤ì¹˜ í•„ìš”] pip install google-generativeai"
    if not key: return "[í‚¤ í•„ìš”] ì˜¤ë¥¸ìª½ ìƒë‹¨ 'ğŸ” Gemini API í‚¤ ì…ë ¥'ì—ì„œ í‚¤ë¥¼ ë„£ì–´ì£¼ì„¸ìš”."
    try:
        genai.configure(api_key=key)
        model = genai.GenerativeModel(model_name=_pick_gemini_model())
        resp = model.generate_content(prompt, generation_config={"temperature":0.4})
        return (resp.text or "").strip() or "[AI ì„¤ëª… ìƒì„± ì‹¤íŒ¨] ì‘ë‹µì´ ë¹„ì—ˆìŠµë‹ˆë‹¤."
    except Exception as e:
        return f"[AI ì„¤ëª… ìƒì„± ì˜¤ë¥˜] {e}"

# -------------------- íƒ­ --------------------
t_overview, t_map, t_store, t_policy = st.tabs(
    ["Overview", "Risk Map", "Store Explorer", "AI Policy Lab"]
)

# -------------------- Overview --------------------
with t_overview:
    st.markdown("### ğŸ§­ ìƒê¶Œ ìœ„í—˜ ê°œìš”")
    lm = latest_month(pred)
    latest = pred[pred["month"]==lm].copy() if lm is not None else pred.copy()
    if not pred.empty:
        any3 = latest.groupby("store_id", observed=False)["risk_label_3m"].max() if "risk_label_3m" in latest else pd.Series(dtype=int)
        any6 = latest.groupby("store_id", observed=False)["risk_label_6m"].max() if "risk_label_6m" in latest else pd.Series(dtype=int)
        total = any3.index.nunique() if len(any3) else pred["store_id"].nunique()
        high3 = int((any3==1).sum()) if len(any3) else 0
        warn6 = int(((any6==1) & ((any3!=1) if len(any3) else True)).sum()) if len(any6) else 0
    else:
        total = high3 = warn6 = 0
    c1,c2,c3,c4 = st.columns(4)
    with c1: st.metric("ì „ì²´ ì í¬ ìˆ˜", num(total))
    with c2: st.metric("3ê°œì›” ë‚´ ê³ ìœ„í—˜ ì í¬", num(high3), pct(high3, total))
    with c3: st.metric("6ê°œì›” ë‚´ ìœ„í—˜ ì í¬", num(warn6), pct(warn6, total))
    with c4:
        f1 = pred.get("f1_3m", pd.Series([np.nan])).dropna(); auc = pred.get("auc_3m", pd.Series([np.nan])).dropna()
        txt = ("F1 " + (f"{f1.iloc[0]:.3f}" if len(f1) else "N/A")) + (f" Â· AUC {auc.iloc[0]:.3f}" if len(auc) else "")
        st.metric("ëª¨ë¸ ì„±ëŠ¥(3M)", txt)

    two = st.columns(2)
    if {"month","risk_proba_3m"}.issubset(pred.columns):
        t = pred.groupby("month", as_index=False, observed=False)["risk_proba_3m"].mean().dropna()
        two[0].markdown(_intro_text("avg3"), unsafe_allow_html=True)
        fig = px.line(t, x="month", y="risk_proba_3m", markers=True, height=360, title="ì›”ë³„ í‰ê·  ìœ„í—˜ í™•ë¥ (3ê°œì›”)")
        fig.update_layout(yaxis_title="ìœ„í—˜ í™•ë¥ (í‰ê· )", xaxis_title="ì›”")
        two[0].plotly_chart(fig, use_container_width=True)
        two[0].markdown(_describe_ts(t["month"], t["risk_proba_3m"], "ì „ì²´ í‰ê· (3ê°œì›”)"), unsafe_allow_html=True)
    if {"month","risk_proba_6m"}.issubset(pred.columns):
        t2 = pred.groupby("month", as_index=False, observed=False)["risk_proba_6m"].mean().dropna()
        two[1].markdown(_intro_text("avg6"), unsafe_allow_html=True)
        fig2 = px.line(t2, x="month", y="risk_proba_6m", markers=True, height=360, title="ì›”ë³„ í‰ê·  ìœ„í—˜ í™•ë¥ (6ê°œì›”)")
        fig2.update_layout(yaxis_title="ìœ„í—˜ í™•ë¥ (í‰ê· )", xaxis_title="ì›”")
        two[1].plotly_chart(fig2, use_container_width=True)
        two[1].markdown(_describe_ts(t2["month"], t2["risk_proba_6m"], "ì „ì²´ í‰ê· (6ê°œì›”)"), unsafe_allow_html=True)

# -------------------- Risk Map --------------------
with t_map:
    st.markdown("### ğŸ—ºï¸ Risk Map â€” ì§€ì—­ë³„ ìœ„í—˜ë„")
    geo_path = APP_DIR / "assets" / "seoul_districts.geojson"
    if pred.empty or "district" not in pred:
        st.info("district ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    else:
        g3 = pred.groupby("district", as_index=False, observed=False)["risk_proba_3m"].mean()
        g6 = pred.groupby("district", as_index=False, observed=False)["risk_proba_6m"].mean() if "risk_proba_6m" in pred else None
        if geo_path.exists():
            import json
            with open(geo_path, encoding="utf-8") as f: geojson = json.load(f)
            which = st.radio("í‘œì‹œ ì§€í‘œ", ["3ê°œì›” ìœ„í—˜ë„(í‰ê· )"] + (["6ê°œì›” ìœ„í—˜ë„(í‰ê· )"] if g6 is not None else []), horizontal=True)
            val_col = "risk_proba_3m" if which.startswith("3ê°œì›”") else "risk_proba_6m"
            g = g3 if val_col=="risk_proba_3m" else g6
            vals = pd.to_numeric(g[val_col], errors="coerce"); vmin, vmax = float(vals.min()), float(vals.max())
            if not (np.isfinite(vmin) and np.isfinite(vmax)) or vmin==vmax: vmin, vmax = 0.0, 1.0
            fig = px.choropleth_mapbox(
                g, geojson=geojson, locations="district", color=val_col, featureidkey="properties.name",
                color_continuous_scale="Reds", range_color=(vmin, vmax),
                mapbox_style="carto-positron", zoom=11.7, center={"lat":37.547,"lon":127.035},
                opacity=0.68, height=680,
                title=("í–‰ì •ë™ë³„ í‰ê·  3ê°œì›” ìœ„í—˜ë„" if val_col=="risk_proba_3m" else "í–‰ì •ë™ë³„ í‰ê·  6ê°œì›” ìœ„í—˜ë„"),
            )
            fig.update_traces(hovertemplate="<b>í–‰ì •ë™:</b> %{location}<br>" +
                              ("<b>3ê°œì›” ìœ„í—˜ë„:</b> %{z:.1%}" if val_col=="risk_proba_3m" else "<b>6ê°œì›” ìœ„í—˜ë„:</b> %{z:.1%}") + "<extra></extra>")
            fig.update_layout(
                hoverlabel=dict(bgcolor="rgba(255,255,255,0.96)", font_size=16, font_color="black", font_family="Arial"),
                margin=dict(l=0,r=0,t=60,b=0),
                coloraxis_colorbar=dict(title="ìœ„í—˜ í™•ë¥ (%)", tickformat=".0%"),
            )
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.warning("ì§€ë„ íŒŒì¼ì´ ì—†ì–´ í‘œë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤. `dashboard/assets/seoul_districts.geojson` ë¥¼ ì¤€ë¹„í•˜ì„¸ìš”.")
            show = g3.merge(g6, on="district", how="outer") if g6 is not None else g3
            if "risk_proba_3m" in show: show["3M(%)"] = (show["risk_proba_3m"]*100).round(1)
            if "risk_proba_6m" in show: show["6M(%)"] = (show["risk_proba_6m"]*100).round(1)
            st.dataframe(show.rename(columns={"district":"í–‰ì •ë™"})[["í–‰ì •ë™"]+[c for c in ["3M(%)","6M(%)"] if c in show]],
                         use_container_width=True, height=580)

# -------------------- Store Explorer --------------------
with t_store:
    st.markdown("### ğŸª ìƒì ëª… ê¸°ë°˜ ìƒì„¸ ë¶„ì„")
    st.caption("í•„í„°ë¡œ í›„ë³´ë¥¼ ì¢íŒ í›„ ì í¬ë¥¼ ì„ íƒí•´ ì£¼ì„¸ìš”. **ì„ íƒ ì í¬ì˜ ìµœì‹  ê¸°ì¤€**ìœ¼ë¡œ ì¶œë ¥ë©ë‹ˆë‹¤.")
    c_dist, c_cat, c_shop = st.columns([1.0, 1.0, 1.4])
    with c_dist:
        dist_sel = st.multiselect("í–‰ì •ë™", sorted(pred["district"].dropna().unique()) if "district" in pred else [], placeholder="í–‰ì •ë™ ì„ íƒ")
    with c_cat:
        cat_sel = st.multiselect("ì—…ì¢…", sorted(pred["category"].dropna().unique()) if "category" in pred else [], placeholder="ì—…ì¢… ì„ íƒ")
    cand = pred.copy()
    if dist_sel and "district" in cand: cand = cand[cand["district"].isin(dist_sel)]
    if cat_sel and "category" in cand: cand = cand[cand["category"].isin(cat_sel)]
    cand_latest = latest_per_store(cand)

    name_col = "MCT_NM_mask" if "MCT_NM_mask" in cand_latest.columns else ("MCT_NM" if "MCT_NM" in cand_latest.columns else None)
    if not name_col: cand_latest["__tmp_name__"] = cand_latest["store_id"].astype(str); name_col = "__tmp_name__"
    def _fmt_label(r):
        nm = str(r.get(name_col,"")).strip(); dong = str(r.get("district","")).strip()
        base = nm if nm else str(r.get("store_id","")); return f"{base} Â· {dong}" if dong else base
    opts = cand_latest[["store_id","district",name_col]].copy(); opts["__label"] = opts.apply(_fmt_label, axis=1)
    dup = opts["__label"].duplicated(keep=False)
    if dup.any(): opts.loc[dup,"__label"] = opts.loc[dup].apply(lambda r: f"{r['__label']} ({str(r['store_id'])[-5:]})", axis=1)
    opts = opts.sort_values("__label"); label_to_id = dict(zip(opts["__label"], opts["store_id"].astype(str)))
    with c_shop:
        sel_label = st.selectbox("ì í¬ ì„ íƒ (ê°€ê²Œëª…)", options=list(label_to_id.keys()), index=0 if len(label_to_id) else None, placeholder="ê°€ê²Œëª… ì„ íƒ")
    if not label_to_id:
        st.warning("ì„ íƒí•œ í–‰ì •ë™/ì—…ì¢…ì— í•´ë‹¹í•˜ëŠ” ì í¬ê°€ ì—†ìŠµë‹ˆë‹¤."); st.stop()

    sel_id = label_to_id[sel_label]; sel_id_str = str(sel_id)
    sel_row_latest = cand_latest[cand_latest["store_id"].astype(str)==sel_id_str].iloc[0]
    st.session_state["sel_store_id"] = sel_id_str
    st.session_state["sel_district"] = str(sel_row_latest.get("district",""))
    st.session_state["sel_category"] = str(sel_row_latest.get("category",""))

    sdf = pred[pred["store_id"].astype(str)==sel_id_str].sort_values("month")
    if sdf.empty: st.info("ì„ íƒí•œ ì í¬ì˜ ì‹œê³„ì—´ ë°ì´í„°ê°€ ì•„ì§ ì—†ì–´ìš”."); st.stop()

    store_disp = str(sel_row_latest.get("MCT_NM", sel_id_str))
    st.markdown(f"#### ğŸ“ ì„ íƒ ì í¬: **{store_disp}**")

    # === ì ìˆ˜(%) ì¤‘ì‹¬ ìƒë‹¨ 3ì¹¸ ===
    cA, cB, cC = st.columns(3)
    last = sdf.iloc[-1]
    proba3 = float(last.get("risk_proba_3m", np.nan))
    proba6 = float(last.get("risk_proba_6m", np.nan)) if "risk_proba_6m" in sdf.columns else np.nan

    # ë³´ì¡° ë“±ê¸‰ í…ìŠ¤íŠ¸ (ê·œì¹™: 3M=1 ì´ë©´ ê³ ìœ„í—˜, ì•„ë‹ˆë©´ 6M=1 ì´ë©´ 'ìœ„í—˜', ë‘˜ë‹¤ 0ì´ë©´ 'ì•ˆì •')
    t3 = int(last.get("risk_label_3m", 0))
    t6 = int(last.get("risk_label_6m", 0))
    tier3 = "ê³ ìœ„í—˜" if t3==1 else "ì•ˆì •"
    tier6 = "ê³ ìœ„í—˜" if t3==1 else ("ìœ„í—˜" if t6==1 else "ì•ˆì •")

    with cA:
        st.metric("3ê°œì›” íì—… ìœ„í—˜ ì ìˆ˜", "N/A" if not np.isfinite(proba3) else f"{proba3*100:.1f}%")
        st.markdown(f"<span class='small'>ë“±ê¸‰: {tier3}</span>", unsafe_allow_html=True)
    with cB:
        st.metric("6ê°œì›” íì—… ìœ„í—˜ ì ìˆ˜", "N/A" if not np.isfinite(proba6) else f"{proba6*100:.1f}%")
        st.markdown(f"<span class='small'>ë“±ê¸‰: {tier6}</span>", unsafe_allow_html=True)
    with cC:
        sdf_local = sdf[["month","risk_proba_3m"]].dropna()
        if len(sdf_local) >= 2:
            w = 3 if len(sdf_local) >= 4 else (len(sdf_local)-1)
            recent_delta = float(sdf_local["risk_proba_3m"].iloc[-1] - sdf_local["risk_proba_3m"].iloc[-(w+1)])
            st.metric("ìµœê·¼ ë³€í™”(3ê°œì›”)", f"{recent_delta*100:+.1f}%p")
        else:
            st.metric("ìµœê·¼ ë³€í™”(3ê°œì›”)", "N/A")

    g1,g2 = st.columns(2)
    if {"month","risk_proba_3m"}.issubset(sdf.columns):
        g1.markdown(_intro_text("shop3"), unsafe_allow_html=True)
        fig = px.line(sdf, x="month", y="risk_proba_3m", markers=True, height=360, title="ì í¬ ìœ„í—˜ í™•ë¥  ì¶”ì„¸ (3ê°œì›”)")
        fig.update_layout(yaxis_title="ìœ„í—˜ í™•ë¥ (ì í¬)", xaxis_title="ì›”"); g1.plotly_chart(fig, use_container_width=True)
        g1.markdown(_describe_ts(sdf["month"], sdf["risk_proba_3m"], "ì´ ì í¬(3ê°œì›”)"), unsafe_allow_html=True)
    if {"month","risk_proba_6m"}.issubset(sdf.columns):
        g2.markdown(_intro_text("shop6"), unsafe_allow_html=True)
        fig2 = px.line(sdf, x="month", y="risk_proba_6m", markers=True, height=360, title="ì í¬ ìœ„í—˜ í™•ë¥  ì¶”ì„¸ (6ê°œì›”)")
        fig2.update_layout(yaxis_title="ìœ„í—˜ í™•ë¥ (ì í¬)", xaxis_title="ì›”"); g2.plotly_chart(fig2, use_container_width=True)
        g2.markdown(_describe_ts(sdf["month"], sdf["risk_proba_6m"], "ì´ ì í¬(6ê°œì›”)"), unsafe_allow_html=True)

    st.markdown("#### âš ï¸ íì—… ì¡°ê¸°ìœ„í—˜ ì›ì¸ ë¶„ì„")
    st.caption("ì•„ë˜ í•­ëª©ì€ ìµœê·¼ ë³€í™”ê°€ í° ì§€í‘œë¥¼ ëª¨ì•„ ë³´ì—¬ë“œë¦½ë‹ˆë‹¤!")
    glabel = _kdict()
    contrib_cols = [c for c in sdf.columns if c.startswith("contrib_") and c.endswith("_3m")]
    top3_groups: list[str] = []
    if contrib_cols:
        lastc = sdf.iloc[-1:][contrib_cols].T; lastc.columns=["val"]
        lastc["group"] = lastc.index.str.replace("contrib_","",regex=False).str.replace("_3m","",regex=False)
        top3_groups = lastc.sort_values("val", ascending=False).head(3)["group"].map(lambda g: glabel.get(g, g)).tolist()
        st.markdown("**ì˜í–¥ ê·¸ë£¹(ìµœì‹ ì›” Top3)**: " + " Â· ".join(top3_groups) if top3_groups else "_-_")
    else:
        st.markdown("<span class='small'>ê·¸ë£¹ ê¸°ì—¬ ì •ë³´ê°€ ì—†ì–´ ìƒëµí•©ë‹ˆë‹¤.</span>", unsafe_allow_html=True)

    # ---- ê²½ê³  ì‚¬ìœ  bullets ìƒì„±
    bullets: list[str] = []
    if not alerts.empty and {"store_id","month"}.issubset(alerts.columns):
        a = alerts.copy(); a["store_id"] = a["store_id"].astype(str).str.strip(); a = a[a["store_id"]==sel_id_str]
        if not a.empty:
            row = a.sort_values("month").iloc[-1]
            for k in ["reason_1","reason_2","reason_3"]:
                txt = str(row.get(k,"")).strip()
                if txt: bullets.append(f"- {txt}")

    if not bullets and (not sigrec.empty) and {"store_id","month"}.issubset(sigrec.columns):
        s = sigrec.copy(); s["store_id"]=s["store_id"].astype(str).str.strip(); s = s[s["store_id"]==sel_id_str].sort_values("month")
        if not s.empty:
            last_s = s.iloc[-1]; candidates = sorted({c[:-8] for c in s.columns if c.endswith("_delta3m")})
            scores = []
            for k in candidates:
                d = pd.to_numeric(last_s.get(f"{k}_delta3m", np.nan), errors="coerce")
                g = pd.to_numeric(last_s.get(f"{k}_peer_gap", np.nan), errors="coerce")
                if not (np.isfinite(d) or np.isfinite(g)): continue
                label_map = {"youth_share":"20ëŒ€ ì´í•˜ ê³ ê° ë¹„ì¤‘","revisit_share":"ì¬ë°©ë¬¸ ê³ ê° ë¹„ì¤‘","new_share":"ì‹ ê·œ ê³ ê° ë¹„ì¤‘",
                             "delivery_ratio":"ë°°ë‹¬ ë§¤ì¶œ ë¹„ì¤‘","peer_sales_ratio":"ë™ì¼ ì—…ì¢… ë§¤ì¶œ ë¹„ìœ¨(=100)",
                             "peer_trx_ratio":"ë™ì¼ ì—…ì¢… ë§¤ì¶œê±´ìˆ˜ ë¹„ìœ¨(=100)","industry_rank_pct":"ì—…ì¢… ë‚´ ë§¤ì¶œ ìˆœìœ„%",
                             "zone_rank_pct":"ìƒê¶Œ ë‚´ ë§¤ì¶œ ìˆœìœ„%","industry_close_ratio":"ì—…ì¢… ë‚´ í•´ì§€ ê°€ë§¹ì  ë¹„ì¤‘",
                             "zone_close_ratio":"ìƒê¶Œ ë‚´ í•´ì§€ ê°€ë§¹ì  ë¹„ì¤‘","resident_share":"ê±°ì£¼ ê³ ê° ë¹„ìœ¨",
                             "worker_share":"ì§ì¥ ê³ ê° ë¹„ìœ¨","floating_share":"ìœ ë™ ê³ ê° ë¹„ìœ¨"}
                negative_is_bad = {"industry_rank_pct","zone_rank_pct","industry_close_ratio","zone_close_ratio"}
                higher_is_better = k not in negative_is_bad
                sign = -1.0 if higher_is_better else +1.0
                score = np.nanmean([sign*(d if np.isfinite(d) else 0.0), sign*(g if np.isfinite(g) else 0.0)])
                scores.append((k, score, d, g, higher_is_better, label_map.get(k,k)))
            scores.sort(key=lambda x: (x[1] if np.isfinite(x[1]) else -1e18), reverse=True)
            for k,_, d,g,good,lab in scores[:3]:
                txt = (f"{lab} í•˜ë½(ìµœê·¼ 3ê°œì›” Î” {d:+.2f}), í–‰ì •ë™Â·ì—…ì¢… ë™ì›” í‰ê·  ëŒ€ë¹„ {g:+.2f}%p"
                       if good else
                       f"{lab} ìƒìŠ¹(ìµœê·¼ 3ê°œì›” Î” {d:+.2f}), í–‰ì •ë™Â·ì—…ì¢… ë™ì›” í‰ê·  ëŒ€ë¹„ {g:+.2f}%p")
                bullets.append(f"- {txt}")

    # âœ… í´ë°±
    used_fallback = False
    if not bullets:
        if contrib_cols:
            gl = _kdict()
            lastc = sdf.iloc[-1:][contrib_cols].T; lastc.columns = ["val"]
            lastc["group"] = lastc.index.str.replace("contrib_","",regex=False).str.replace("_3m","",regex=False)
            top = (lastc.sort_values("val", ascending=False).head(3)["group"].map(lambda g: gl.get(g, g)).tolist())
            bullets = [f"- {g} ê´€ë ¨ ì§€í‘œê°€ ìµœê·¼ì›”ì— ë†’ì€ ì˜í–¥" for g in top]
            used_fallback = True
        else:
            sdf_local = sdf[["month","risk_proba_3m"]].dropna()
            if len(sdf_local) >= 2:
                ch = float(sdf_local["risk_proba_3m"].iloc[-1] - sdf_local["risk_proba_3m"].iloc[-2])
                bullets = [f"- ìµœê·¼ 1ê°œì›” ìœ„í—˜í™•ë¥  ë³€í™”: {ch*100:+.1f}%p"]
                used_fallback = True

    if bullets:
        st.markdown("**ğŸ“Œ ì²´í¬ í¬ì¸íŠ¸**")
        if used_fallback:
            st.caption("íŒŒì¼ì´ ì—†ê±°ë‚˜ ë¹„ì–´ ìˆì–´ ìë™ ìƒì„±ëœ ìš”ì•½ì…ë‹ˆë‹¤.")
        st.markdown("\n".join(bullets))
    else:
        st.info("ì„¤ëª…ì— í™œìš© ê°€ëŠ¥í•œ ì‹ í˜¸ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. (alerts/sigrec íŒŒì¼ ë˜ëŠ” ê´€ë ¨ ì»¬ëŸ¼ ìƒì„± í•„ìš”)")

    col_ai1, _ = st.columns([1,3])
    try: ai_toggle = col_ai1.toggle("AI ì„¤ëª… ì¼œê¸° ğŸ”", value=False)
    except Exception: ai_toggle = col_ai1.checkbox("AI ì„¤ëª… ì¼œê¸° ğŸ”", value=False)

    def _friendly_tone(text: str) -> str:
        for a,b in {"ì•…í™”":"ì•„ì‰¬ìš´ íë¦„","ê¸‰ê°":"í° ì¡°ì •","ê°ì†Œ":"ì¡°ì •","í•˜ë½":"ì¡°ì •","ë¬¸ì œ":"ê³¼ì œ"}.items():
            text = text.replace(a,b)
        return text

    if ai_toggle:
        if _HAS_GEMINI: _ = _get_gemini_key_from_user()
        store_name = str(sel_row_latest.get("MCT_NM", sel_id_str))
        district = st.session_state.get("sel_district",""); category = st.session_state.get("sel_category","")
        sdf_local = sdf[["month","risk_proba_3m"]].dropna()
        recent_delta = np.nan
        if len(sdf_local) >= 2:
            w = 3 if len(sdf_local) >= 4 else (len(sdf_local)-1)
            recent_delta = float(sdf_local["risk_proba_3m"].iloc[-1] - sdf_local["risk_proba_3m"].iloc[-(w+1)])
        extra_metrics = {}
        if np.isfinite(proba3): extra_metrics["ìœ„í—˜í™•ë¥ _í˜„ì¬(3M)"] = f"{proba3*100:.1f}%"
        if np.isfinite(recent_delta): extra_metrics["ìµœê·¼ë³€í™”(3M)"] = f"{recent_delta*100:+.1f}%p"

        prompt = build_ai_prompt(
            store_name=store_name, store_id=sel_id_str,
            district=district, category=category,
            top_groups=top3_groups, reasons=bullets,
            score_now=proba3 if np.isfinite(proba3) else np.nan,
            extra_metrics=extra_metrics,
        )

        with st.spinner("AIê°€ ì í¬ ìƒí™©ì„ ì •ë¦¬í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
            ai_text = _friendly_tone(_gemini_generate(prompt))

        st.markdown("**âœ¨ AI ìš”ì•½**")
        parts = [s.strip(" -â€¢Â·\t.") for s in ai_text.replace("â€¢","\n").replace("Â·","\n").split("\n") if s.strip(" -â€¢Â·\t.")]
        items = parts[:3]
        if items:
            st.markdown("<ul style='margin-top:0.4rem; line-height:1.6;'>"+ "".join(f"<li>{it}</li>" for it in items)+"</ul>", unsafe_allow_html=True)
        else:
            st.caption("ìƒì„±ëœ ë¬¸ì¥ì´ ì—†ìŠµë‹ˆë‹¤.")
        _save_ai_log(sel_id_str, prompt, ai_text, extra_metrics)

# -------------------- AI Policy Lab --------------------
with t_policy:
    st.markdown("### ğŸ’¡ AI Policy Lab â€” ìœ„í—˜ìœ í˜•ë³„ ë§ì¶¤ ì•¡ì…˜ & ì •ì±… ì¶”ì²œ")
    if pred.empty: st.info("ì˜ˆì¸¡ íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤."); st.stop()
    st.markdown("##### ì í¬ ì„ íƒ")
    c_dist2, c_cat2, c_shop2 = st.columns([1.0,1.0,1.4])
    with c_dist2:
        dist_sel2 = st.multiselect("í–‰ì •ë™", sorted(pred["district"].dropna().unique()) if "district" in pred else [],
                                   default=[st.session_state.get("sel_district","")] if st.session_state.get("sel_district") else None)
    with c_cat2:
        cat_sel2 = st.multiselect("ì—…ì¢…", sorted(pred["category"].dropna().unique()) if "category" in pred else [],
                                  default=[st.session_state.get("sel_category","")] if st.session_state.get("sel_category") else None)
    cand2 = pred.copy()
    if dist_sel2 and "district" in cand2: cand2 = cand2[cand2["district"].isin(dist_sel2)]
    if cat_sel2 and "category" in cand2: cand2 = cand2[cand2["category"].isin(cat_sel2)]
    cand2_latest = latest_per_store(cand2)
    name_col2 = "MCT_NM_mask" if "MCT_NM_mask" in cand2_latest.columns else ("MCT_NM" if "MCT_NM" in cand2_latest.columns else None)
    if not name_col2: cand2_latest["__tmp_name__"] = cand2_latest["store_id"].astype(str); name_col2 = "__tmp_name__"
    def _fmt_label2(r):
        nm = str(r.get(name_col2,"")).strip(); dong = str(r.get("district","")).strip()
        base = nm if nm else str(r.get("store_id","")); return f"{base} Â· {dong}" if dong else base
    opts2 = cand2_latest[["store_id","district",name_col2]].copy(); opts2["__label"] = opts2.apply(_fmt_label2, axis=1)
    dup2 = opts2["__label"].duplicated(keep=False)
    if dup2.any(): opts2.loc[dup2,"__label"] = opts2.loc[dup2].apply(lambda r: f"{r['__label']} ({str(r['store_id'])[-5:]})", axis=1)
    opts2 = opts2.sort_values("__label"); label_to_id2 = dict(zip(opts2["__label"], opts2["store_id"].astype(str)))
    default_index = 0
    if st.session_state.get("sel_store_id") and label_to_id2:
        try: default_index = list(label_to_id2.values()).index(st.session_state["sel_store_id"])
        except ValueError: default_index = 0
    with c_shop2:
        sel_label2 = st.selectbox("ì í¬ ì„ íƒ (ê°€ê²Œëª…)", options=list(label_to_id2.keys()),
                                  index=default_index if len(label_to_id2) else None)
    if not label_to_id2: st.warning("ì„ íƒí•œ í–‰ì •ë™/ì—…ì¢…ì— í•´ë‹¹í•˜ëŠ” ì í¬ê°€ ì—†ìŠµë‹ˆë‹¤."); st.stop()

    sel_id2 = label_to_id2[sel_label2]; sel_id_str2 = str(sel_id2)
    row_latest2 = cand2_latest[cand2_latest["store_id"].astype(str)==sel_id_str2].iloc[0]
    district = str(row_latest2.get("district","")); category = str(row_latest2.get("category",""))
    _last_line2 = pred[pred["store_id"].astype(str)==sel_id_str2].sort_values("month").iloc[-1]
    # risk_tier ì—†ìœ¼ë©´ ë ˆì´ë¸”ë¡œ ê³„ì‚°
    if "risk_tier" in _last_line2.index:
        risk_tier = str(_last_line2.get("risk_tier","ì•ˆì •"))
    else:
        _t3 = int(_last_line2.get("risk_label_3m",0)); _t6 = int(_last_line2.get("risk_label_6m",0))
        risk_tier = "ê³ ìœ„í—˜" if _t3==1 else ("ìœ„í—˜" if _t6==1 else "ì•ˆì •")
    st.session_state.update({"sel_store_id":sel_id_str2,"sel_district":district,"sel_category":category,"sel_risk_tier":risk_tier})

    sdf2 = pred[pred["store_id"].astype(str)==sel_id_str2].sort_values("month")
    if sdf2.empty: st.info("ì„ íƒí•œ ì í¬ì˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."); st.stop()

    c1,c2,c3 = st.columns(3)
    c1.metric("í˜„ì¬ ë“±ê¸‰", risk_tier); c2.metric("í–‰ì •ë™", district or "-"); c3.metric("ì—…ì¢…", category or "-")

    tabs = st.tabs(["ì •ì±… ì¶”ì²œ","ê¸ˆìœµ/ë³´í—˜ ì œì•ˆ","ë§ˆì¼€íŒ…/ê³ ê°í™•ì¥","ê³µë™êµ¬ë§¤/ì›ê°€ì ˆê°"])
    if policy_map.empty:
        for t in tabs:
            with t: st.info("ì •ì±… ë§¤í•‘ íŒŒì¼ì´ ì—†ì–´ ë°ëª¨ ì¹´ë“œë¡œ ëŒ€ì²´ë©ë‹ˆë‹¤. outputs í´ë”ì˜ ì •ì±… ë§¤í•‘ xlsx íŒŒì¼ëª…ì„ í™•ì¸í•˜ì„¸ìš”.")
    else:
        def show_cards(df):
            if df.empty: st.info("ì¶”ì²œ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤."); return
            top = df.head(8)
            for _, r in top.iterrows():
                with st.container(border=True):
                    st.markdown(f"**{r.get('title','(ë¬´ì œ)')}**  \n"
                                f"<span class='small'>{r.get('owner','')}</span>", unsafe_allow_html=True)
                    if str(r.get("summary","")).strip(): st.caption(str(r.get("summary","")).strip())
                    cols = st.columns(4)
                    cols[0].write(f"ìœ í˜•: **{r.get('support_type','-')}**")
                    cols[1].write(f"ì§€ì—­: **{r.get('region','-')}**")
                    cols[2].write(f"ì—…ì¢…: **{r.get('industry','-')}**")
                    ddl = r.get("deadline"); cols[3].write(f"ë§ˆê°: **{ddl.date() if pd.notna(ddl) else 'ìƒì‹œ'}**")
                    url = str(r.get("url","")).strip()
                    if url: st.link_button("ì‹ ì²­/ì•ˆë‚´ ë°”ë¡œê°€ê¸°", url)
        with tabs[0]: st.subheader("ì •ë¶€/ì§€ìì²´ ì •ì±… ì¶”ì²œ"); show_cards(policy_map[policy_map["support_type"].isin(["policy","grant","advisory","subsidy","gov","public"])])
        with tabs[1]: st.subheader("ê¸ˆìœµ/ë³´í—˜ ì œì•ˆ"); show_cards(policy_map[policy_map["support_type"].isin(["loan","credit","bnpl","insurance","fintech"])])
        with tabs[2]: st.subheader("ë§ˆì¼€íŒ…/ê³ ê°í™•ì¥"); show_cards(policy_map[policy_map["support_type"].isin(["marketing","coupon","ad","growth"])])
        with tabs[3]:
            st.subheader("ê³µë™êµ¬ë§¤/ì›ê°€ì ˆê°")
            show_cards(policy_map[policy_map["support_type"].isin(["sourcing","procurement","costdown","rent"])])

            # === â¬‡ï¸ ì´ë¯¸ì§€ ì°¸ê³ í•œ 'ê³µë™êµ¬ë§¤ & ë¦¬ì›Œë“œ' ì¶”ê°€ ì¹´ë“œ ===
            # === ê³µë™êµ¬ë§¤ & ë¦¬ì›Œë“œ (í…ìŠ¤íŠ¸ ì„¤ëª…í˜•) ===
            st.markdown("#### ê³µë™êµ¬ë§¤ & ë¦¬ì›Œë“œ")
            
            with st.container(border=True):
                st.markdown(
                    """
            ##### í”„ë¡œê·¸ë¨ êµ¬ì¡°
            - **ë°ì´í„° ê¸°ë°˜ ë§¤ì¹­**: ìœ„í—˜ìƒê¶ŒÂ·ì†Œìƒê³µì¸ ë°ì´í„°ë¥¼ í™œìš©í•´ ì í•©í•œ ê³µê¸‰ì‚¬ë¥¼ ìë™ ë§¤ì¹­
            - **ì§€ìì²´Â·ì¹´ë“œì‚¬ í˜‘ì—…**: ì„±ë™êµ¬ Ã— ì‹ í•œì¹´ë“œì™€ í•¨ê»˜ **ì‹ìì¬ ê³µë™êµ¬ë§¤** ìš´ì˜  
              Â· **ê³ ìœ„í—˜ë„ ì í¬ ìš°ì„  ì•Œë¦¼/ì„ ì°©ìˆœ ê¸°íšŒ ë¶€ì—¬**
            - **AI ê³µë™êµ¬ë§¤ ê·¸ë£¹**: ë™ì¼ ì—…ì¢…Â·ì§€ì—­ ì í¬ë¥¼ ë¬¶ì–´ **AI ê¸°ë°˜ ê³µë™êµ¬ë§¤ ê·¸ë£¹** ìë™ êµ¬ì„±
            - **ê²°ì œÂ·í¬ì¸íŠ¸ ì—°ê³„**: ì¹´ë“œê²°ì œ ë° ì œíœ´ í¬ì¸íŠ¸ì™€ ì—°ë™
            
            ##### ë¦¬ì›Œë“œ(í¬ì¸íŠ¸) ì •ì±…
            - **ê³ ê° ìœ ì§€Â·ì¬ë°©ë¬¸ ì‹œ í¬ì¸íŠ¸ ì ë¦½** (ì í¬/ì¹´ë“œì‚¬ ê³µë™ ìš´ì˜)
            - **ê³µë™êµ¬ë§¤ ê²°ì œ ì‹œ í¬ì¸íŠ¸ ì‚¬ìš©** ê°€ëŠ¥ (í˜„ê¸ˆì„± í• ì¸ íš¨ê³¼)
            - **ì œíœ´ í¬ì¸íŠ¸ í†µí•©**: ì˜ˆ) ì‹ í•œí¬ì¸íŠ¸ ë“±ê³¼ ìƒí˜¸ ì „í™˜/ì‚¬ìš©
            
            ##### ìˆ˜ìµëª¨ë¸
            - **ê±°ë˜ ìˆ˜ìˆ˜ë£Œ**: B2B íŒë§¤ì•¡ì˜ **1â€“2%**
            - **í”„ë¦¬ë¯¸ì—„ ê³µê¸‰ì‚¬ ë“±ë¡ë¹„**: ìƒë‹¨ ë…¸ì¶œÂ·ì…ì°°ê¶Œ ë“± ë¶€ê°€ í˜œíƒ í¬í•¨
            - **ì„±ê³¼ê³µìœ (ì„±ê³µë³´ìˆ˜)**: **ë¹„ìš©ì ˆê°ì•¡ì˜ ì¼ì • ë¹„ìœ¨**ì„ ì„±ê³µë³´ìˆ˜ë¡œ ìˆ˜ì·¨
            
            ##### ë¹„ì¦ˆë‹ˆìŠ¤ ë¼ì¸
            - **ê³µë™êµ¬ë§¤Â·ë¹„ìš©ì ˆê° (Shared Sourcing)**  
              ì›ì¬ë£ŒÂ·í¬ì¥ì¬Â·ì„œë¹„ìŠ¤ë¥¼ ê³µë™êµ¬ë§¤í•˜ëŠ” **B2B ìƒìƒ ë§ˆì¼“í”Œë ˆì´ìŠ¤** êµ¬ì¶•  
              Â· **íŒë§¤ìˆ˜ìˆ˜ë£Œ/í”Œë«í¼ ê±°ë˜ ìˆ˜ìµ** Â· **í˜‘ë ¥ì—…ì²´ ë‚©í’ˆê³„ì•½ ìˆ˜ìµ**
                    """,
                    unsafe_allow_html=True,
                )

